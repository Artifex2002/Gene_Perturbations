{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f888e127-b9eb-4633-8487-921a1b46e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /net/dali/home/mscbio/ahk112/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahk112\u001b[0m (\u001b[33mahk112-university-of-pittsburgh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "# grelu enformer really wants this wandb bs so did that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046fcb25-2d7d-4643-b42e-6bb08bf3c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# grelu is installed like: pip3 install grelu\n",
    "from grelu.model.models import EnformerPretrainedModel\n",
    "from grelu.sequence.format import convert_input_type\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5720a197-ce8b-4307-9212-2e4e458c4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data_processed\"\n",
    "FASTA_PATH = os.path.join(base_dir, \"promoters_flank1000.fasta\")   # our promoter fasta\n",
    "EMB_OUT_PATH = os.path.join(base_dir, \"enformer_promoter_embeddings_flank1000.npy\")\n",
    "GENE_OUT_PATH = os.path.join(base_dir, \"enformer_promoter_genes_flank1000.npy\")\n",
    "\n",
    "pairs_path = os.path.join(base_dir, \"tf_gene_pairs_with_promoters_flank1000.csv\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7c4284-fcbd-4b16-b529-b970594fdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LENGTH = 2000   # length we want per sequence (promoter is ±1000bp)\n",
    "BATCH_SIZE = 32        # can bump up/down depending on GPU mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c2cb54-c2a6-4c32-8d46-c4e9842b005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fasta into gene names and sequences. headers are gene names followed by seqs\n",
    "def read_fasta(path):\n",
    "    gene_names = []\n",
    "    sequences = []\n",
    "    curr_name = None\n",
    "    curr_seq = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                # add prev record\n",
    "                if curr_name is not None:\n",
    "                    gene_names.append(curr_name)\n",
    "                    sequences.append(\"\".join(curr_seq))\n",
    "                curr_name = line[1:].strip()\n",
    "                curr_seq = []\n",
    "            else:\n",
    "                curr_seq.append(line)\n",
    "    # last record\n",
    "    if curr_name is not None:\n",
    "        gene_names.append(curr_name)\n",
    "        sequences.append(\"\".join(curr_seq))\n",
    "    return gene_names, sequences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae25a23-4538-4327-a846-e940b2b1e335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#genes: 4828\n",
      "First 5 gene names: ['A1BG', 'AAK1', 'AAMDC', 'AAMP', 'AARSD1']\n",
      "First seq length: 2000\n"
     ]
    }
   ],
   "source": [
    "gene_names, sequences = read_fasta(FASTA_PATH)\n",
    "print(f\"#genes: {len(gene_names)}\")\n",
    "print(\"First 5 gene names:\", gene_names[:5])\n",
    "print(\"First seq length:\", len(sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b62eac-1f6c-4e88-9d3b-0f494aa715a7",
   "metadata": {},
   "source": [
    "above should have ~5k genes and seq length of 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd413d1-c95a-4e57-b94c-a4d330b5f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min len: 2000 Max len: 2000\n"
     ]
    }
   ],
   "source": [
    "# padding to target length just in case (enformer wants fixed lengths)\n",
    "\n",
    "def pad_or_trim(seq, target_len):\n",
    "    if len(seq) > target_len:\n",
    "        return seq[:target_len]\n",
    "    elif len(seq) < target_len:\n",
    "        return seq + \"N\" * (target_len - len(seq)) # n universally means any base\n",
    "    else:\n",
    "        return seq\n",
    "\n",
    "padded_seqs = [pad_or_trim(s, TARGET_LENGTH) for s in sequences]\n",
    "\n",
    "lengths = [len(s) for s in padded_seqs]\n",
    "print(\"Min len:\", min(lengths), \"Max len:\", max(lengths)) # should be 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214b6da0-c193-404c-938d-502fe6288077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of one-hot items: 4828\n",
      "Shape of first one-hot: (4, 2000)\n"
     ]
    }
   ],
   "source": [
    "# OHC sequences for enformer. something like 4 or 5 (cuz N) by 2k\n",
    "ohes = convert_input_type(\n",
    "    inputs=padded_seqs,\n",
    "    output_type=\"one_hot\",   # ask for one-hot encoding\n",
    "    genome=\"hg38\",           # we are passing seqs so maybe don't need\n",
    "    add_batch_axis=False,    # we want [L, 4] per item; DataLoader will add batch dim\n",
    ")\n",
    "print(\"Number of one-hot items:\", len(ohes))\n",
    "print(\"Shape of first one-hot:\", np.array(ohes[0]).shape)\n",
    "# should be something like (TARGET_LENGTH, 4). 4 or 5 for the latter cuz N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d99add55-5bf3-4151-91b8-2f34ec8593d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohes_np shape: (4828, 4, 2000)\n"
     ]
    }
   ],
   "source": [
    "# OH seqs in a dataloader to batch into enformer. just the one hot arrays\n",
    "\n",
    "# list into a numpy array so it is treated as a dataset of arrays\n",
    "ohes_np = np.stack(ohes, axis=0)   # shape: (N, L, 4)\n",
    "print(\"ohes_np shape:\", ohes_np.shape)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=ohes_np,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9c94cb-6163-477d-85be-fc13ba483fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous parameter to wandb.login() has no effect and will be removed in future versions.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'human_state_dict:latest', 939.29MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:15.0 (62.5MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enformer params: 229,943,840\n"
     ]
    }
   ],
   "source": [
    "# load enformer from grelu (pretrained) \n",
    "feature_extractor = EnformerPretrainedModel(\n",
    "    n_tasks=32,    \n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "feature_extractor.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in feature_extractor.parameters())\n",
    "print(\"Enformer params:\", f\"{total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ca9e22-2504-40b8-9654-c81cfa6877b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (4828, 32)\n",
      "Should match #genes: 4828 vs 4828\n"
     ]
    }
   ],
   "source": [
    "# run the enformer. one embedding per seq\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.float().to(device) # batch is B, L, 4/5\n",
    "        out = feature_extractor(batch)\n",
    "\n",
    "        out = out.squeeze(-1)\n",
    "\n",
    "        all_embeddings.append(out.cpu().numpy())\n",
    "\n",
    "embeddings = np.concatenate(all_embeddings, axis=0) #shape is N,D\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Should match #genes:\", embeddings.shape[0], \"vs\", len(gene_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ff5447d-8fbd-4809-b0c9-a5fd6c4fccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to: data_processed/enformer_promoter_embeddings_flank1000.npy\n",
      "Saved gene names to: data_processed/enformer_promoter_genes_flank1000.npy\n"
     ]
    }
   ],
   "source": [
    "# save embeddings\n",
    "np.save(EMB_OUT_PATH, embeddings)\n",
    "np.save(GENE_OUT_PATH, np.array(gene_names))\n",
    "\n",
    "print(\"Saved embeddings to:\", EMB_OUT_PATH)\n",
    "print(\"Saved gene names to:\", GENE_OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0219903-9db7-422a-90f4-c4071660b77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             TF     gene      expr  \\\n",
      "606847      BID    RPL7A -0.050565   \n",
      "695173    SALL3     SGCB  0.216774   \n",
      "87447    NFATC1  SMARCE1  0.050626   \n",
      "58264   NEUROD1   MRPS24 -0.227060   \n",
      "441685     PAX5   ZNF428  0.017421   \n",
      "\n",
      "                                                      emb  \n",
      "606847  [-0.08855927, -0.019950302, 0.15034984, -0.002...  \n",
      "695173  [-0.005865831, 0.099577, -0.008437029, 0.01797...  \n",
      "87447   [0.059702277, 0.06322225, 0.0258956, -0.084927...  \n",
      "58264   [-0.2606159, 0.023753757, 0.14891303, -0.12770...  \n",
      "441685  [0.060610272, -0.22239193, -0.25339863, 0.0353...  \n"
     ]
    }
   ],
   "source": [
    "# how to put these into the tf-gene pair df. don't save into that file cuz it would be massive\n",
    "# here we just build a dictionary mapping gene to embedding and fetch on the go\n",
    "pairs = pd.read_csv(pairs_path)\n",
    "\n",
    "emb = np.load(EMB_OUT_PATH)\n",
    "genes_emb = np.load(GENE_OUT_PATH)\n",
    "\n",
    "gene_to_idx = {g: i for i, g in enumerate(genes_emb)} # gene -> row index map\n",
    "\n",
    "# helper to get embedding for a gene\n",
    "def get_emb(gene):\n",
    "    idx = gene_to_idx.get(gene)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    return emb[idx]\n",
    "\n",
    "# test on a subset of 5\n",
    "pairs_sample = pairs.sample(5, random_state=0).copy()\n",
    "pairs_sample[\"emb\"] = pairs_sample[\"gene\"].apply(get_emb)\n",
    "\n",
    "print(pairs_sample[[\"TF\", \"gene\", \"expr\", \"emb\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
